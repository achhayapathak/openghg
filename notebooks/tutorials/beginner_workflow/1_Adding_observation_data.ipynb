{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow: processing, searching and retrieving observations\n",
    "\n",
    "This tutorial demonstrates how OpenGHG can be used to process new measurement data, search the data present and to retrieve this for analysis and visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check installation\n",
    "\n",
    "This tutorial assumes that you have installed `openghg`. To ensure install has been successful you can open an `ipython` console and try to import this module.\n",
    "\n",
    "In a terminal type:\n",
    "\n",
    "```bash\n",
    "$ ipython\n",
    "```\n",
    "\n",
    "Then import `openghg` and print the version string associated with the version you have installed. If you get something like the below `openghg` is installed correctly.\n",
    "\n",
    "```ipython\n",
    "In [1]: import openghg\n",
    "In [2]: openghg.__version__\n",
    "Out[2]: '0.0.1'\n",
    "```\n",
    "\n",
    "If you get an ``ImportError`` please go back to the [install section of the documentation](https://docs.openghg.org/install.html).\n",
    "\n",
    "### Jupyter notebooks\n",
    "\n",
    "If you haven't used Jupyter notebooks before please see [this introduction](https://realpython.com/jupyter-notebook-introduction/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up an object store\n",
    "\n",
    "The OpenGHG platform uses what's called an *object store* to save data. Any saved data has been processed into a standardised format, assigned universally unique identifiers (UUIDs) and stored alongside associated metadata (such as site and species details). Storing data in this way allows for fast retrieval and efficient searching.\n",
    "\n",
    "When using OpenGHG on a local machine the location of the object store is set using an `OPENGHG_PATH` environment variable (explained below) and this can be any directory on your local system.\n",
    "\n",
    "For this tutorial, we will create a temporary object store which we can add data to. This path is fine for this purpose but as it is a temporary directory it may not survive a reboot of the computer. \n",
    "\n",
    "The `OPENGHG_PATH` environment variable can be set up in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:35.115393Z",
     "iopub.status.busy": "2022-02-01T15:30:35.114707Z",
     "iopub.status.idle": "2022-02-01T15:30:35.117712Z",
     "shell.execute_reply": "2022-02-01T15:30:35.118116Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "os.environ[\"OPENGHG_PATH\"] = tmp_dir.name   # temporary directory\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating your own longer term object store we recommend a path such as ``~/openghg_store`` which will create the object store in your home directory in a directory called ``openghg_store``. If you want this to be a permanent location this can be added to your \"~/.bashrc\" or \"~/.bash_profile\" file depending on the system being used. e.g. as\n",
    "\n",
    "```bash\n",
    " export OPENGHG_PATH=\"$HOME/openghg_store\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding and standardising data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types\n",
    "\n",
    "Within OpenGHG there are several data types which can be processed and stored within the object store. This includes data from the AGAGE, DECC, NOAA, LondonGHG, BEAC2ON networks.\n",
    "\n",
    "When uploading a new data file, the data type must be specified alongside some additional details so OpenGHG can recognise the format and the correct standardisation can occur. The details needed will vary by the type of data being uploaded but will often include the measurement reference (e.g. a site code) and the name of any network.\n",
    "\n",
    "For the full list of accepted observation inputs and data types, there is a summary function which can be called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:35.122312Z",
     "iopub.status.busy": "2022-02-01T15:30:35.121776Z",
     "iopub.status.idle": "2022-02-01T15:30:36.699540Z",
     "shell.execute_reply": "2022-02-01T15:30:36.699953Z"
    }
   },
   "outputs": [],
   "source": [
    "from openghg.standardise import summary_data_types\n",
    "\n",
    "summary = summary_data_types()\n",
    "\n",
    "## UNCOMMENT THIS CODE TO SHOW ALL ENTRIES\n",
    "# import pandas as pd; pd.set_option('display.max_rows', None)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there may be multiple data types applicable for a give site. This is can be dependent on various factors including the instrument type used to measure the data e.g. for Tacolneston (\"TAC\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:36.709613Z",
     "iopub.status.busy": "2022-02-01T15:30:36.709007Z",
     "iopub.status.idle": "2022-02-01T15:30:36.712568Z",
     "shell.execute_reply": "2022-02-01T15:30:36.712077Z"
    }
   },
   "outputs": [],
   "source": [
    "summary[summary[\"Site code\"] == \"TAC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DECC network\n",
    "\n",
    "We will start by adding data to the object store from a surface site within the DECC network. Here we have accessed a subset of data from the Tacolneston site (site code \"TAC\") in the UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:36.716251Z",
     "iopub.status.busy": "2022-02-01T15:30:36.715762Z",
     "iopub.status.idle": "2022-02-01T15:30:36.717894Z",
     "shell.execute_reply": "2022-02-01T15:30:36.718283Z"
    }
   },
   "outputs": [],
   "source": [
    "from openghg.util import retrieve_example_data\n",
    "\n",
    "tac_data = retrieve_example_data(path=\"timeseries/tac_example.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this data is measured in-situ, this is classed as a surface site and we need to use the `ObsSurface` class to interpret this data. We can pass our list of files to the `read_file` method associated within the `ObsSurface` class, also providing details on:\n",
    " - site code - `\"TAC\"` for Billsdale\n",
    " - type of data we want to process, known as the data type - `\"CRDS\"`\n",
    " - network - `\"DECC\"`\n",
    "\n",
    "This is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:36.723148Z",
     "iopub.status.busy": "2022-02-01T15:30:36.722650Z",
     "iopub.status.idle": "2022-02-01T15:30:37.507161Z",
     "shell.execute_reply": "2022-02-01T15:30:37.507561Z"
    }
   },
   "outputs": [],
   "source": [
    "from openghg.client import process_files\n",
    "\n",
    "decc_results = process_files(files=tac_data, data_type=\"CRDS\", site=\"TAC\", network=\"DECC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:37.511546Z",
     "iopub.status.busy": "2022-02-01T15:30:37.510995Z",
     "iopub.status.idle": "2022-02-01T15:30:37.513452Z",
     "shell.execute_reply": "2022-02-01T15:30:37.513871Z"
    }
   },
   "outputs": [],
   "source": [
    "print(decc_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here this extracts the data (and metadata) from the supplied files, standardises them and adds these to our created object store.\n",
    "\n",
    "The returned `decc_results` will give us a dictionary of how the data has been stored. The data itself may have been split into different entries, each one stored with a unique ID (UUID). Each entry is known as a *Datasource* (see below for a note on Datasources). The `decc_results` output includes details of the processed data and tells us that the data has been stored correctly. This will also tell us if any errors have been encountered when trying to access and standardise this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGAGE data\n",
    "\n",
    "Another data type which can be added is data from the AGAGE network. The functions that process the AGAGE data expect data to have an accompanying precisions file. For each data file we create a tuple with the data filename and the precisions filename. *Note: A simpler method of uploading these file types is planned.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve the example data for Capegrim as we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capegrim_data = retrieve_example_data(path=\"timeseries/capegrim_example.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capegrim_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must create a `tuple` associated with each data file to link this to a precision file:\n",
    "\n",
    "```python\n",
    "list_of_tuples = [(data1_filepath, precision1_filepath), (data2_filepath, precision2_filepath), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capegrim_data.sort()\n",
    "capegrim_tuple = (capegrim_data[0], capegrim_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data being uploaded here is from the Cape Grim station in Australia, site code \"CGO\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add these files to the object store in the same way as the DECC data by including the right keywords:\n",
    " - site code - `\"CGO\"` for Cape Grim\n",
    " - data type - `\"GCWERKS\"`\n",
    " - network - `\"AGAGE\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:37.523809Z",
     "iopub.status.busy": "2022-02-01T15:30:37.523017Z",
     "iopub.status.idle": "2022-02-01T15:30:39.416566Z",
     "shell.execute_reply": "2022-02-01T15:30:39.416958Z"
    }
   },
   "outputs": [],
   "source": [
    "agage_results = process_files(files=capegrim_tuple, data_type=\"GCWERKS\", site=\"CGO\", \n",
    "                              network=\"AGAGE\", instrument=\"medusa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewing `agage_results` there will be a large number of Datasource UUIDs shown due to the large number of gases in each data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:39.422370Z",
     "iopub.status.busy": "2022-02-01T15:30:39.421800Z",
     "iopub.status.idle": "2022-02-01T15:30:39.424511Z",
     "shell.execute_reply": "2022-02-01T15:30:39.424887Z"
    }
   },
   "outputs": [],
   "source": [
    "agage_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on Datasources\n",
    "\n",
    "Datasources are objects that are stored in the object store (++add link to object store notes++) that hold the data and metadata associated with each measurement we upload to the platform.\n",
    "\n",
    "For example, if we upload a file that contains readings for three gas species from a single site at a specific inlet height OpenGHG    will assign this data to three different Datasources, one for each species. Metadata such as the site, inlet height, species, network etc are stored alongside the measurements for easy searching. \n",
    "\n",
    "Datasources can also handle multiple versions of data from a single site, so if scales or other factors change multiple versions may be stored for easy future comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Searching for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the object store\n",
    "\n",
    "Now that we have added data to our created object store, we can view the objects within it in a simple force graph model. To do this we use the `view_store` function from the `objectstore` submodule. Note that the cell may take a few moments to load as the force graph is created.\n",
    "\n",
    "In the force graph the central blue node is the `ObsSurface` node. Associated with this node are all the data processed by it. The next node in the topology are networks, shown in green. In the graph you will see `DECC` and `AGAGE` nodes from the data files we have added. From these you'll see site nodes in red and then individual datasources in orange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The object store visualisation created by this function is commented out here and won't be visible in the documentation but can be uncommented and run when you use the notebook version.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:39.428494Z",
     "iopub.status.busy": "2022-02-01T15:30:39.427995Z",
     "iopub.status.idle": "2022-02-01T15:30:39.431118Z",
     "shell.execute_reply": "2022-02-01T15:30:39.430627Z"
    }
   },
   "outputs": [],
   "source": [
    "from openghg.objectstore import visualise_store\n",
    "\n",
    "# visualise_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know we have this data in the object store we can search it and retrieve data from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching the object store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search the object store by property using the `search(...)` function.\n",
    "\n",
    "For example we can find all sites which have measurements for carbon tetrafluoride (\"cf4\") using the `species` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:39.435272Z",
     "iopub.status.busy": "2022-02-01T15:30:39.434780Z",
     "iopub.status.idle": "2022-02-01T15:30:39.496363Z",
     "shell.execute_reply": "2022-02-01T15:30:39.495936Z"
    }
   },
   "outputs": [],
   "source": [
    "from openghg.client import search\n",
    "\n",
    "search(species=\"cfc11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look for details of all the data measured at the Billsdale (\"BSD\") site using the `site` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:39.500355Z",
     "iopub.status.busy": "2022-02-01T15:30:39.499860Z",
     "iopub.status.idle": "2022-02-01T15:30:39.578596Z",
     "shell.execute_reply": "2022-02-01T15:30:39.578092Z"
    }
   },
   "outputs": [],
   "source": [
    "search(site=\"tac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this site you can see this contains details of each of the species as well as the inlet heights these were measured at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the standardised data from the object store there are several functions we can use which depend on the type of data we want to access.\n",
    "\n",
    "To access the surface data we have added so far we can use the `get_obs_surface` function and pass keywords for the site code, species and inlet height to retrieve our data.\n",
    "\n",
    "In this case we want to extract the carbon dioxide (\"co2\") data from the Tacolneston data (\"TAC\") site measured at the \"185m\" inlet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:39.582641Z",
     "iopub.status.busy": "2022-02-01T15:30:39.582144Z",
     "iopub.status.idle": "2022-02-01T15:30:39.721657Z",
     "shell.execute_reply": "2022-02-01T15:30:39.721059Z"
    }
   },
   "outputs": [],
   "source": [
    "from openghg.client import get_obs_surface\n",
    "\n",
    "co2_data = get_obs_surface(site=\"tac\", species=\"co2\", inlet=\"185m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we view our returned `obs_data` variable this will contain:\n",
    "\n",
    " - `data` - The standardised data (accessed using e.g. `obs_data.data`). This is returned as an [xarray Dataset](https://xarray.pydata.org/en/stable/generated/xarray.Dataset.html).\n",
    " - `metadata` - The associated metadata (accessed using e.g. `obs_data.metadata`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:39.731216Z",
     "iopub.status.busy": "2022-02-01T15:30:39.730659Z",
     "iopub.status.idle": "2022-02-01T15:30:39.734136Z",
     "shell.execute_reply": "2022-02-01T15:30:39.733633Z"
    }
   },
   "outputs": [],
   "source": [
    "co2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a simple plot using the `plot_timeseries` method of the `ObsData` object.\n",
    "\n",
    "> **_NOTE:_**  the plot created below may not show up on the online documentation version of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_data.plot_timeseries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass any of `title`, `xlabel`, `ylabel` and `units` to the `plot_timeseries` function to modify the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used the `tmp_dir` as a location for your object store at the start of the tutorial you can run the cell below to remove any files that were created to make sure any persistant data is refreshed when the notebook is re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-01T15:30:39.802003Z",
     "iopub.status.busy": "2022-02-01T15:30:39.801113Z",
     "iopub.status.idle": "2022-02-01T15:30:39.809732Z",
     "shell.execute_reply": "2022-02-01T15:30:39.809269Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_dir.cleanup()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
