{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUGS - HUb for Greenhouse gas data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUGS works in a modular way without a fixed hierarchical structure. \n",
    "\n",
    "There are multiple modules\n",
    "\n",
    "* Datasources\n",
    "* Instrument\n",
    "* Sites\n",
    "* Networks\n",
    "\n",
    "For example they can be arranged as\n",
    "\n",
    "* Network\n",
    "    * Site\n",
    "        * Instrument\n",
    "            * Datasource\n",
    "            * Datasource\n",
    "            * Datasource\n",
    "\n",
    "Or\n",
    "            \n",
    "\n",
    "* Network\n",
    "    * Datasource\n",
    "    * Datasource\n",
    "    * Datasource\n",
    "\n",
    "There is no set fixed hierarchy for these modules\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is stored within objects and these objects may hold links to data in the object store.\n",
    "\n",
    "In the following code we will read in a data file, analyse the data contained within it, segment the data into sections which can be easily stored in the object store and then recombine these dataframes ready for export to the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress some Pandas warnings - these will be fixed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Data pretty printer for nicer printing of data\n",
    "import pprint\n",
    "# User PrettyPrinter to print them in a nicer way\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For listing of objects in the object store\n",
    "from objectstore.hugs_objstore import list_object_names\n",
    "# To get the local bucket (a container for data in the object store)\n",
    "from objectstore.local_bucket import get_local_bucket\n",
    "# The object to process and store CRDS data\n",
    "from processing._crds import CRDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data\n",
    "\n",
    "Here we read in a data file from the Bilsdale site using the read_file() function from the CRDS class\n",
    "\n",
    "This function\n",
    "* Creates a CRDS object\n",
    "* Collects metadata\n",
    "* Splits the data into separate dataframes for each gas\n",
    "* Creates a Datasource object for the gas, holding data and metadata\n",
    "* Stores this data within the CRDS object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/bsd.picarro.1minute.248m.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crds = CRDS.read_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the daterange for the data read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crds.get_daterange()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the function has processed the datafile correctly and picked up the right dates I've used the Linux `head` and `tail` applications to get the first and last rows (not containing NaNs) from the data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head\n",
    "\n",
    "`140130 105230       air    8   1960.24   0.236    26    409.66   0.028    26    204.62   6.232    26`\n",
    "\n",
    "Tail\n",
    "\n",
    "`140130 142030       air    8   1952.24   0.674    25    408.78   0.019    25    196.35   6.879    25`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view some of the data stored in each Datasource within this CRDS object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = crds.get_datasources()\n",
    "\n",
    "for d in datasources:\n",
    "    data = d.get_data()\n",
    "    # Print the top two lines of each dataframe\n",
    "    print(\"\\n\", data.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the metadata collected. This is stored within a Metadata object within the CRDS object and we can now view this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = crds.get_metadata()\n",
    "pp.pprint(metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "We can now visualize some of this data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Get the first datasource\n",
    "datasource = datasources[0]\n",
    "for datasource in datasources:\n",
    "    data = datasource.get_data()\n",
    "    # Get column names\n",
    "    col_names = list(data.columns)\n",
    "    ax = data.plot(x=\"Datetime\", y=col_names[1], elinewidth=1, linewidth=0, marker=\"o\", markersize=3, \n",
    "                   yerr=col_names[2], legend=None, color=\"#59a14f\", title=datasource._name.upper())\n",
    "    ax.set(xlabel=\"Time\", ylabel=\"Count\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed and segmented data is now ready to be stored in the object store.\n",
    "\n",
    "The data for each gas is stored within a Pandas DataFrame that gets converted into a compressed binary HDF5 file format similar to that used by NetCDF. Each datasource stores its data with a key containing its own universally unique identifier (UUID)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object has a .save() function that saves the object to the data store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a bucket, a container used to store data in the object store\n",
    "bucket = get_local_bucket(empty=True)\n",
    "# Save the object to the object store\n",
    "crds.save(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for this save function is given below\n",
    "\n",
    "```py\n",
    "def save(self, bucket=None):\n",
    "    \"\"\" Save the object to the object store\n",
    "\n",
    "        Args:\n",
    "            bucket (dict, default=None): Bucket for data\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    if self.is_null():\n",
    "        return\n",
    "\n",
    "    # If a bucket isn't passed, get the hugs bucket\n",
    "    if bucket is None:\n",
    "        bucket = _get_bucket()\n",
    "    \n",
    "    # Create the key at which to store this object\n",
    "    crds_key = \"%s/uuid/%s\" % (CRDS._crds_root, self._uuid)\n",
    "    \n",
    "    # Get the datasources to save themselves to the object store\n",
    "    for d in self._datasources:\n",
    "        d.save(bucket)\n",
    "    \n",
    "    # Save this object as JSON\n",
    "    _ObjectStore.set_object_from_json(bucket=bucket, key=crds_key, data=self.to_data())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function then saves the CRDS object and each of its Datasources to the object store. \n",
    "As each Datasource holds gas data this in turn is saved by the save function of each Datasource.\n",
    "\n",
    "We can now view the structure of the object store by querying the keys in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the objects in the container\n",
    "bucket_list = list_object_names(bucket)\n",
    "\n",
    "pp.pprint(bucket_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object in the object store is saved at a key. This allows each object to be stored at a unique location.\n",
    "\n",
    "Objects are stored as so\n",
    "\n",
    "`{object_name}/uuid/{uuid}`\n",
    "\n",
    "Objects that contain dates are given the key\n",
    "\n",
    "`{object_name}/uuid/{uuid}/{startdatetime_enddateime}`\n",
    "\n",
    "For example data stored in a Datasource at key\n",
    "\n",
    "`datasource/uuid/8338eb59-8eac-4b6c-8331-61d68e0e2ed3`\n",
    "\n",
    "is given the key\n",
    "\n",
    "`data/uuid/8338eb59-8eac-4b6c-8331-61d68e0e2ed3/2014-01-30T10:52:30_2014-01-30T14:20:30`\n",
    "\n",
    "Notice that the UUIDs in both keys are the same.\n",
    "\n",
    "Some objects can also be accessed by name through their name key\n",
    "\n",
    "`{object_name}/name/{name}/{uuid}`\n",
    "\n",
    "This allows lookup of an object's UUID by its name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look for data in the object store by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing._crds import CRDS\n",
    "\n",
    "# Get the search start date\n",
    "# As this datafile only has data for a single day we just use the same start and end datetime\n",
    "start = CRDS.to_datetime(\"2014-01-30\")\n",
    "end =  CRDS.to_datetime(\"2014-01-30\")\n",
    "\n",
    "object_type = \"datasource\"\n",
    "# We can now search the object store for keys\n",
    "keys = crds.search_store(bucket=bucket, root_path=object_type, datetime_begin=start, datetime_end=end)\n",
    "\n",
    "pp.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the keys for data in the object store that holds data between those dates. Currently as datafiles are not large they are not being split into week/month segments depending on the resolution of the readings. This feature will be implemented soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the keys for the Datasources containing this data we can recombine these pieces into a single \n",
    "dataframe. Here we will choose to use all three.\n",
    "\n",
    "Functions to select and order data within the produced dataframe will be implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing._recombination import get_sections\n",
    "\n",
    "# Get the data at each of the found keys\n",
    "datasources = get_sections(bucket, keys)\n",
    "\n",
    "print(datasources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [datasource.get_data() for datasource in datasources]\n",
    "\n",
    "for d in dataframes:\n",
    "    print(\"\\n\",d.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing._recombination import combine_sections\n",
    "\n",
    "# Combine each of the sections into a single dataframe\n",
    "combined = combine_sections(dataframes)\n",
    "\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now have a look at this data again and make sure it comes out correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = list(combined.columns)\n",
    "\n",
    "print(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = combined.plot(x=\"Datetime\", y=\"ch4 count\", elinewidth=1, linewidth=0, marker=\"o\", markersize=3, \n",
    "               yerr=\"ch4 stdev\", legend=None, color=\"#4e79a7\", title=\"CH4\")\n",
    "\n",
    "ax = combined.plot(x=\"Datetime\", y=\"co2 count\", elinewidth=1, linewidth=0, marker=\"o\", markersize=3, \n",
    "               yerr=\"co2 stdev\", legend=None, color=\"#59a14f\", title=\"CO2\")\n",
    "\n",
    "ax = combined.plot(x=\"Datetime\", y=\"co count\", elinewidth=1, linewidth=0, marker=\"o\", markersize=3, \n",
    "               yerr=\"co stdev\", legend=None, color=\"#e15759\", title=\"CO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now seen data being processed, metadata extracted and saved in the object store. It was then recombined from the object store and exported as a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
